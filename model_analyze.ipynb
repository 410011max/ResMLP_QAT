{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from src.models import *\n",
    "from src.model_analysis.utils import same_output\n",
    "from src.post_quant.cle import cle_for_resmlp\n",
    "from src.post_quant.bias_absorb import ba_for_resmlp\n",
    "from src.model_analysis.visualize import layer_dist, act_dist, simulate_input, scale_plot, add_value_labels\n",
    "\n",
    "# org_model = resmlp_24(pretrained=False).cuda().eval()\n",
    "model = resmlp_24(pretrained=False).eval()\n",
    "model.load_state_dict(torch.load(\"ResMLP_S24_ReLU_fp32_80.602.pth\"))\n",
    "\n",
    "model_norm = resmlp_24_norm(pretrained=False).eval()\n",
    "model_norm.load_state_dict(torch.load(\"qat_norm.pth\")[\"model\"])\n",
    "# qmodel = q_resmlp(model)\n",
    "\n",
    "# # test CLE\n",
    "# org_model = resmlp_24(pretrained=False).eval()\n",
    "# org_model.load_state_dict(torch.load(\"ResMLP_S24_ReLU_fp32_80.602.pth\"))\n",
    "# model = resmlp_24(pretrained=False).eval()\n",
    "# model.load_state_dict(torch.load(\"ResMLP_S24_ReLU_fp32_80.602.pth\"))\n",
    "# cle_for_resmlp(model)\n",
    "# print(same_output(model, org_model, eps=1e-5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qmodel_org = q_resmlp(model)\n",
    "qmodel_org.load_state_dict(torch.load(\"resmlp_org.pth\", map_location='cpu')['state_dict'], strict=False)\n",
    "qmodel_org.blocks[0].add_1.mult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for a in torch.load(\"resmlp_norm.pth\", map_location='cpu')['state_dict']:\n",
    "    if a.startswith(\"blocks.0.add_1.mult\"):\n",
    "        print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.load(\"resmlp_norm.pth\", map_location='cpu')['state_dict']['blocks.0.norm1.scale']\n",
    "torch.tensor([x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ckpt = torch.load(\"resmlp_norm.pth\", map_location='cpu')\n",
    "\n",
    "# for i in range(0, 24):\n",
    "#     for b in range(0,2):\n",
    "#         ckpt['state_dict'][f'blocks.{i}.norm{b+1}.scale'] = torch.tensor([ckpt['state_dict'][f'blocks.{i}.norm{b+1}.scale']])\n",
    "\n",
    "# torch.save(ckpt, \"resmlp_norm.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qmodel_norm = q_resmlp_norm(model_norm)\n",
    "qmodel_norm.load_state_dict(torch.load(\"resmlp_norm.pth\", map_location='cpu')['state_dict'], strict=False)\n",
    "qmodel_org.blocks[0].add_1.mult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.round(torch.tensor([-12 / 2**3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def orgDivRound(x, s):\n",
    "    return torch.round(x / s + 0.000001)\n",
    "\n",
    "def orgDivp5Floor(x, s):\n",
    "    return torch.floor(x / s + 0.5)\n",
    "\n",
    "def round(x):\n",
    "    return torch.round(x + 0.000001)\n",
    "\n",
    "def p5Floor(x):\n",
    "    return torch.floor(x + 0.5)\n",
    "\n",
    "def eDivRound(x, m, e):\n",
    "    fix = torch.bitwise_left_shift(1, e-1)\n",
    "    mul = x * m\n",
    "    return torch.bitwise_right_shift(mul + fix, e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = -15\n",
    "print(x / qmodel_org.blocks[0].add_1.s)\n",
    "print(orgDivRound(x, qmodel_org.blocks[0].add_1.s))\n",
    "print(orgDivp5Floor(x, qmodel_org.blocks[0].add_1.s))\n",
    "# print(eDivRound(x, qmodel_org.blocks[0].add_1.mult, qmodel_org.blocks[0].add_1.shift))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([12])\n",
    "print(x/ 2**3)\n",
    "print(orgDivRound(x, 2**3))\n",
    "print(orgDivp5Floor(x, 2**3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([1.5])\n",
    "print(x)\n",
    "print(round(x))\n",
    "print(p5Floor(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([-1.5])\n",
    "print(torch.round(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2,1, figsize=(20, 10))\n",
    "ax[0].get_xaxis().set_visible(False)\n",
    "ax[1].get_xaxis().set_visible(False)\n",
    "\n",
    "# cle_for_resmlp(model)\n",
    "layer_dist(model, 0, 23, name=\"ResMLP-org: Weight Distribution (after CLE)\", ax=ax[0])\n",
    "# cle_for_resmlp(model)\n",
    "# res_\n",
    "# cle_for_resmlp(model_norm)\n",
    "layer_dist(model_norm, 0, 23, name=\"ResMLP-BN: Weight Distribution (before CLE)\", ax=ax[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2,1, figsize=(20, 10))\n",
    "ax[0].get_xaxis().set_visible(False)\n",
    "ax[1].get_xaxis().set_visible(False)\n",
    "\n",
    "model.cuda()\n",
    "model_norm.cuda()\n",
    "act_dist(model, 0, 23, name='ResMLP-org: Activation Distribution of Each Layer', ax=ax[0], real_sim=True)\n",
    "act_dist(model_norm, 0, 23, name='ResMLP-BN: Activation Distribution of Each Layer', ax=ax[1], real_sim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = resmlp_24(pretrained=True).eval()\n",
    "fig, ax = plt.subplots(2,1, figsize=(20, 10))\n",
    "ax[0].get_xaxis().set_visible(False)\n",
    "ax[1].get_xaxis().set_visible(False)\n",
    "\n",
    "# model.cuda()\n",
    "# model_norm.cuda()\n",
    "model.cpu()\n",
    "model_norm.cpu()\n",
    "labels, data = scale_plot(model, 0, 23, show_layers=[\"gamma_1\", \"gamma_2\"], name='Scale of gamma_1, gamma_2', ax=ax[0])\n",
    "for label in (ax[0].get_xticklabels() + ax[0].get_yticklabels()):\n",
    "    label.set_fontsize(16)\n",
    "ax[0].plot(labels, data)\n",
    "add_value_labels(ax[0], skip_cnt=1, precision=2)\n",
    "\n",
    "labels, data = scale_plot(model_norm, 0, 23, show_layers=[\"gamma_1\", \"gamma_2\"], name='Scale of gamma_1, gamma_2', ax=ax[1])\n",
    "for label in (ax[1].get_xticklabels() + ax[1].get_yticklabels()):\n",
    "    label.set_fontsize(16)\n",
    "ax[1].plot(labels, data)\n",
    "add_value_labels(ax[1], skip_cnt=1, precision=2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = resmlp_24(pretrained=True).eval()\n",
    "fig, ax = plt.subplots(2,1, figsize=(20, 10))\n",
    "ax[0].get_xaxis().set_visible(False)\n",
    "ax[1].get_xaxis().set_visible(False)\n",
    "\n",
    "act_dist(model_norm, 0, 23, name='ResMLP-BN: Activation Distribution of Each Layer', ax=ax[0])\n",
    "\n",
    "act_dist(model_norm, 0, 23, name='ResMLP-BN: Activation Distribution of Each Layer (without attn)', show_layers=[\"mlp.fc1\", \"mlp.fc2\"], ax=ax[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = resmlp_24(pretrained=True).eval()\n",
    "fig, ax = plt.subplots(2,1, figsize=(20, 10))\n",
    "ax[0].get_xaxis().set_visible(False)\n",
    "ax[1].get_xaxis().set_visible(False)\n",
    "\n",
    "act_dist(model, 0, 23, name='Activation Distribution of Each Layer', ax=ax[0])\n",
    "\n",
    "act_dist(model_norm, 0, 23, name='ResMLP-BN: Activation Distribution of Each Layer', ax=ax[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ckp = torch.load(\"./resmlp_norm2.pth\", map_location='cpu')[\"model\"]\n",
    "# modified_dict = {}\n",
    "# for k, v in ckp.items():\n",
    "#     # if \"norm\" in k:\n",
    "#     #     # print(k)\n",
    "#     #     if (\"weight\" in k) or (\"bias\" in k):\n",
    "#     #         continue\n",
    "#     modified_dict[k] = v\n",
    "\n",
    "# _ = model_norm.load_state_dict(ckp, strict=False)\n",
    "\n",
    "checkpoint = torch.load(\"./resmlp_norm2.pth\", map_location='cpu')[\"model\"]\n",
    "_ = model_norm.load_state_dict(checkpoint, strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_norm = resmlp_24_norm(pretrained=True).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchsummary as summary\n",
    "summary.summary(model, (3, 224, 224), device=\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchsummary as summary\n",
    "summary.summary(model_norm, (3, 224, 224), device=\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getattr(model, f\"layer0\")\n",
    "b_temp = None\n",
    "for i in range(1):\n",
    "    for n, m in model_norm.blocks[i].named_modules():\n",
    "        print(n)\n",
    "        if \"norm1\" in n:\n",
    "            b_temp = m.cpu().eval()\n",
    "            # n_parameters = sum(p.numel() for p in m.parameters() if p.requires_grad)\n",
    "            # print(n_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = 2#196\n",
    "L = 5#384\n",
    "\n",
    "# x = np.array(np.random.rand(C, L), dtype=np.float32)\n",
    "# x = torch.from_numpy(x).unsqueeze(0)\n",
    "\n",
    "\n",
    "# x = torch.tensor([[[1., 2., 3., 4., 5.],\n",
    "#          [10., 20., 30., 40., 50.]]])\n",
    "x = torch.tensor([[[1., 2., 3., 4., 5.], [2., 4., 6., 8., 10.]], [[10., 20., 30., 40., 50.], [20., 30., 40., 50., 60.]]])\n",
    "print(x.shape)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.transpose(1,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qqq = x.transpose(1,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qqq - torch.mean(qqq, dim=(0, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# linear = nn.Linear(L, L, bias=True)\n",
    "bn = nn.BatchNorm1d(5, momentum=1, affine=True)\n",
    "\n",
    "# x = linear(x)\n",
    "x = torch.tensor([[[1., 2., 3., 4., 5.], [2., 4., 6., 8., 10.]], [[10., 20., 30., 40., 50.], [20., 30., 40., 50., 60.]], [[1., 2., 3., 4., 5.], [2., 4., 6., 8., 10.]], [[10., 20., 30., 40., 50.], [20., 30., 40., 50., 60.]]])\n",
    "# print(x.transpose(1,2).shape, '\\n', x.transpose(1,2))\n",
    "\n",
    "_ = bn(x.transpose(1,2)).transpose(1,2)\n",
    "\n",
    "# # bn(x.transpose(1,2)).transpose(1,2)\n",
    "# # bn(x.transpose(1,2)).transpose(1,2)\n",
    "# print(bn.running_mean, bn.running_var)\n",
    "# print(torch.mean(x, dim=(0, 1)), torch.var(x, dim=(0, 1)))\n",
    "\n",
    "bn.eval()\n",
    "print(bn(x.transpose(1,2)).transpose(1,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bn.running_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "std = (bn.running_var + bn.eps).sqrt()\n",
    "t = torch.diag(bn.weight / std)\n",
    "b = bn.weight * bn.running_mean / std\n",
    "(x @ t) - b + bn.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "std = (bn.running_var + bn.eps).sqrt()\n",
    "b = bn.running_mean / std\n",
    "x * std + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _fuse_bn(self, conv_or_fc, bn):\n",
    "    std = (bn.running_var + bn.eps).sqrt()\n",
    "    t = bn.weight / std\n",
    "    if conv_or_fc.weight.ndim == 4:\n",
    "        t = t.reshape(-1, 1, 1, 1)\n",
    "    else:\n",
    "        t = t.reshape(-1, 1)\n",
    "    return conv_or_fc.weight * t, bn.bias - bn.running_mean * bn.weight / std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = (x - bn.running_mean) * torch.rsqrt(bn.running_var + bn.eps)\n",
    "\n",
    "#bn.weight * z + bn.bias\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([[[1., 2., 3., 4., 5.], [2., 4., 6., 8., 10.]], [[10., 20., 30., 40., 50.], [20., 30., 40., 50., 60.]]])\n",
    "a = bn(x.transpose(1,2)).transpose(1,2)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bn.running_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fuse_linear_bn_weights(linear_w, linear_b, bn_rm, bn_rv, bn_eps, bn_w, bn_b):\n",
    "    if linear_b is None:\n",
    "        linear_b = torch.zeros_like(bn_rm)\n",
    "    bn_scale = bn_w * torch.rsqrt(bn_rv + bn_eps)\n",
    "\n",
    "    fused_w = linear_w * bn_scale.unsqueeze(-1)\n",
    "    fused_b = (linear_b - bn_rm) * bn_scale + bn_b\n",
    "\n",
    "    return torch.nn.Parameter(fused_w), torch.nn.Parameter(fused_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fuse_1(linear, bn):\n",
    "    w = linear.weight\n",
    "    print(w.size())\n",
    "    mean = bn.running_mean\n",
    "    var = bn.running_var\n",
    "    eps = bn.eps\n",
    "    beta = bn.weight\n",
    "    gamma = bn.bias\n",
    "    if linear.bias is not None:\n",
    "        b = linear.bias\n",
    "    else:\n",
    "        b = torch.zeros_like(mean.shape)\n",
    "\n",
    "    \n",
    "    bn_scale = beta * torch.rsqrt(var + eps)\n",
    "    w = w * bn_scale#.unsqueeze(-1)\n",
    "    b = (b - mean)*bn_scale + gamma\n",
    "    fused_linear = nn.Linear(linear.in_features, linear.out_features)\n",
    "                                             \n",
    "    fused_linear.weight = nn.Parameter(w)\n",
    "    fused_linear.bias = nn.Parameter(b)\n",
    "    return fused_linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_l = fuse_1(linear, bn)\n",
    "new_l(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class M(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(1, 1)\n",
    "        self.bn1d = nn.BatchNorm1d(1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear(x)\n",
    "        x = self.bn1d(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.ao.quantization.quantize_fx import (\n",
    "    prepare_fx,\n",
    "    convert_fx,\n",
    "    prepare_qat_fx,\n",
    "    fuse_fx,\n",
    ")\n",
    "\n",
    "m = M().eval()\n",
    "m = fuse_fx(m)\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w, b = fuse_linear_bn_weights(linear.weight, linear.bias, bn.running_mean, bn.running_var, bn.eps, bn.weight, bn.bias)\n",
    "F.linear(x, w, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bn.running_mean\n",
    "bn.running_var\n",
    "bn.weight\n",
    "bn.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "denom = torch.sqrt(bn.running_var + bn.eps)\n",
    "b = bn.bias - bn.weight * bn.running_mean / denom\n",
    "a = bn.weight / denom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F.linear(x, torch.eye(196).mul_(a), b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F.linear(x.transpose(1,2), torch.diag(a), b).transpose(1,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = x.mean(dim=0).unsqueeze(dim=0)\n",
    "var = ((x-mean)**2).mean(dim=0).unsqueeze(dim=0)\n",
    "out = (x - mean / torch.sqrt(var + 1e-5))\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = resmlp_24(pretrained=True).cuda().eval()\n",
    "model_norm = resmlp_24_norm(pretrained=True).cuda().eval()\n",
    "fig, ax = plt.subplots(2,1, figsize=(20, 10))\n",
    "ax[0].get_xaxis().set_visible(False)\n",
    "ax[1].get_xaxis().set_visible(False)\n",
    "\n",
    "act_dist(model, 0, 23, name='ResMLP-org: Activation Distribution of Each Layer', ax=ax[0], real_sim=False)\n",
    "act_dist(model_norm, 0, 23, name='ResMLP-BN: Activation Distribution of Each Layer', ax=ax[1], real_sim=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bn.running_mean.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x - bn.running_mean) / torch.sqrt(bn.running_var + bn.eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bn.weight * (x - bn.running_mean) / torch.sqrt(bn.running_var + bn.eps) + bn.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_factor = bn.weight / torch.sqrt(bn.running_var + bn.eps)\n",
    "weight = torch.diag(output_factor)\n",
    "bias = - output_factor * bn.running_mean + bn.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F.linear(x, weight, bias)#.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_norm.blocks[0].gamma_2.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_temp.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "running_std = torch.sqrt(b_temp.running_var + b_temp.eps)\n",
    "scale_factor = b_temp.weight / running_std\n",
    "scale_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "running_std = torch.sqrt(self.bn.running_var.detach() + self.bn.eps)\n",
    "            scale_factor = self.bn.weight / running_std\n",
    "            scaled_weight = self.conv.weight * scale_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHECKPOINT_PATH = \"input8-conv-res16-act8.pth.tar\"\n",
    "\n",
    "model = resmlp_24()\n",
    "qmodel = q_resmlp(model)\n",
    "qmodel.load_state_dict(torch.load(CHECKPOINT_PATH)['state_dict'])\n",
    "\n",
    "for n, m in qmodel.named_modules():\n",
    "    if isinstance(m, QLinear):\n",
    "        print(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = resmlp_24(pretrained=False).eval()\n",
    "qmodel = q_resmlp(model)\n",
    "qmodel.load_state_dict(torch.load(CHECKPOINT_PATH)['state_dict'])\n",
    "fig, ax = plt.subplots(2,1, figsize=(20, 10))\n",
    "ax[0].get_xaxis().set_visible(False)\n",
    "ax[1].get_xaxis().set_visible(False)\n",
    "\n",
    "label_1 = []\n",
    "data_1 = []\n",
    "for n, m in qmodel.blocks.named_modules():\n",
    "    if \"add_1.observer\" in n:\n",
    "        label_1.append(n)\n",
    "        data_1.append([m.scale.item()])\n",
    "\n",
    "ax[0].set_title('Rescale After add_1 (fp)', size=30)\n",
    "ax[0].tick_params(labelrotation=30)\n",
    "for label in (ax[0].get_xticklabels() + ax[0].get_yticklabels()):\n",
    "    label.set_fontsize(16)\n",
    "ax[0].plot(label_1, data_1)\n",
    "add_value_labels(ax[0], skip_cnt=2, precision=4)\n",
    "\n",
    "label_2 = []\n",
    "data_2 = []\n",
    "for n, m in qmodel.named_modules():\n",
    "    if \"add_2.observer\" in n:\n",
    "        label_2.append(n)\n",
    "        data_2.append(m.scale.item())\n",
    "\n",
    "ax[1].set_title('Rescale After add_2 (fp)', size=30)\n",
    "ax[1].tick_params(labelrotation=30)\n",
    "for label in (ax[1].get_xticklabels() + ax[1].get_yticklabels()):\n",
    "    label.set_fontsize(16)\n",
    "ax[1].plot(label_2, data_2)\n",
    "add_value_labels(ax[1], skip_cnt=2, precision=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = resmlp_24(pretrained=False).eval()\n",
    "qmodel = q_resmlp(model)\n",
    "qmodel.load_state_dict(torch.load(CHECKPOINT_PATH)['state_dict'])\n",
    "fig, ax = plt.subplots(2,1, figsize=(20, 10))\n",
    "ax[0].get_xaxis().set_visible(False)\n",
    "ax[1].get_xaxis().set_visible(False)\n",
    "\n",
    "label_1 = []\n",
    "data_1 = []\n",
    "for n, m in qmodel.blocks.named_modules():\n",
    "    if \"add\" in n and \"observer\" not in n:\n",
    "        label_1.append(n)\n",
    "        scale = (m.res_mult[0].type(torch.double) / (2.0 ** m.res_shift[0]).type(torch.double)).type(torch.float)\n",
    "        data_1.append(scale)\n",
    "\n",
    "ax[0].set_title('Alignment for Residual Layer (int16 -> int32)', size=30)\n",
    "ax[0].tick_params(labelrotation=30)\n",
    "for label in (ax[0].get_xticklabels() + ax[0].get_yticklabels()):\n",
    "    label.set_fontsize(16)\n",
    "ax[0].plot(label_1, data_1)\n",
    "add_value_labels(ax[0], skip_cnt=1, precision=2)\n",
    "\n",
    "\n",
    "label_2 = []\n",
    "data_2 = []\n",
    "for n, m in qmodel.named_modules():\n",
    "    if \"add\" in n and \"observer\" not in n:\n",
    "        label_2.append(n)\n",
    "        scale = (m.mult[0].type(torch.double) / (2.0 ** m.shift[0]).type(torch.double)).type(torch.float)\n",
    "        data_2.append(scale)\n",
    "\n",
    "ax[1].set_title('Rescale After Residual Connection (int32 -> int8)', size=30)\n",
    "ax[1].tick_params(labelrotation=30)\n",
    "for label in (ax[1].get_xticklabels() + ax[1].get_yticklabels()):\n",
    "    label.set_fontsize(16)\n",
    "ax[1].plot(label_2, data_2)\n",
    "add_value_labels(ax[1], skip_cnt=2, precision=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = resmlp_24(pretrained=False).eval()\n",
    "qmodel = q_resmlp(model)\n",
    "qmodel.load_state_dict(torch.load(CHECKPOINT_PATH)['state_dict'])\n",
    "fig, ax = plt.subplots(2,1, figsize=(20, 10))\n",
    "ax[0].get_xaxis().set_visible(False)\n",
    "ax[1].get_xaxis().set_visible(False)\n",
    "\n",
    "label_1 = []\n",
    "data_1 = []\n",
    "for n, m in qmodel.named_modules():\n",
    "    if isinstance(m, QLinear):\n",
    "        label_1.append(n)\n",
    "        data_1.append(m.observer.scale.item())\n",
    "\n",
    "ax[0].set_title('Scales for Weights Layers (fp -> int8)', size=30)\n",
    "ax[0].tick_params(labelrotation=30)\n",
    "for label in (ax[0].get_xticklabels() + ax[0].get_yticklabels()):\n",
    "    label.set_fontsize(16)\n",
    "ax[0].plot(label_1, data_1)\n",
    "add_value_labels(ax[0], skip_cnt=9, precision=3)\n",
    "\n",
    "\n",
    "# label_2 = []\n",
    "# data_2 = []\n",
    "# for n, m in qmodel.named_modules():\n",
    "#     if \"add\" in n and \"observer\" not in n:\n",
    "#         label_2.append(n)\n",
    "#         scale = (m.mult[0].type(torch.double) / (2.0 ** m.shift[0]).type(torch.double)).type(torch.float)\n",
    "#         data_2.append(scale)\n",
    "\n",
    "# ax[1].set_title('Rescale After Residual Connection (int32 -> int8)', size=30)\n",
    "# ax[1].tick_params(labelrotation=30)\n",
    "# for label in (ax[1].get_xticklabels() + ax[1].get_yticklabels()):\n",
    "#     label.set_fontsize(16)\n",
    "# ax[1].plot(label_2, data_2)\n",
    "# add_value_labels(ax[1], skip_cnt=2, precision=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7b7296a56d372591cb5ced48c9a892695554f23e2f1788c0250eea3690b4c0d6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
