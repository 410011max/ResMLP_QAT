{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.datasets as datasets\n",
    "from bit_config import *\n",
    "from utils import *\n",
    "import torch\n",
    "from torchsummary import summary\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import os, time\n",
    "\n",
    "model = resmlp_24(pretrained=True)\n",
    "qmodel = q_resmlp24(model, full_precision_flag=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensor shape test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T1 = torch.tensor([[1, 2, 3],\n",
    "                [4, 5, 6],\n",
    "                [7, 8, 9]])\n",
    "\n",
    "torch.abs(T1).max(axis = 0)[0]   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weight/Bias Modification Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_linear_layers(model):\n",
    "    linear_layers = []\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, QuantLinear):\n",
    "            linear_layers.append((name, module))\n",
    "    return linear_layers\n",
    "\n",
    "# get_linear_layers(qmodel.layer0)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_linear_layers(model):\n",
    "    linear_layers = []\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, nn.Linear):\n",
    "            linear_layers.append((name, module))\n",
    "    return linear_layers\n",
    "\n",
    "get_linear_layers(model.blocks[0])[0][1].weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(5, 384, 196)\n",
    "b_std_mean = torch.std_mean(x, dim=[0, 1], unbiased=False)\n",
    "print(x.shape, b_std_mean[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_layer_equalization(linear_layers):\n",
    "    '''\n",
    "    Perform Cross Layer Scaling :\n",
    "    Iterate modules until scale value is converged up to 1e-8 magnitude\n",
    "    '''\n",
    "    S_history = dict()\n",
    "    eps = 1e-8\n",
    "    converged = [False] * (len(linear_layers)-1)\n",
    "    with torch.no_grad(): \n",
    "        while not np.all(converged):\n",
    "            for idx in range(1, len(linear_layers)):\n",
    "                (prev_name, prev), (curr_name, curr) = linear_layers[idx-1], linear_layers[idx]\n",
    "                \n",
    "                range_1 = 2.*torch.abs(prev.weight).max(axis = 1)[0] # abs max of each row * 2\n",
    "                range_2 = 2.*torch.abs(curr.weight).max(axis = 0)[0] # abs max of each col * 2\n",
    "                S = torch.sqrt(range_1 * range_2) / range_2\n",
    "\n",
    "                if idx in S_history:\n",
    "                    prev_s = S_history[idx]\n",
    "                    if torch.allclose(S, prev_s, atol=eps):\n",
    "                        converged[idx-1] = True\n",
    "                        continue\n",
    "                    else:\n",
    "                        converged[idx-1] = False\n",
    "\n",
    "                # div S for each row\n",
    "                prev.weight.data.div_(S.view(-1, 1))\n",
    "                if prev.bias is not None:\n",
    "                    prev.bias.data.div_(S)\n",
    "                \n",
    "                # mul S for each col\n",
    "                curr.weight.data.mul_(S)\n",
    "                    \n",
    "                S_history[idx] = S\n",
    "    return linear_layers\n",
    "\n",
    "def cle_for_resmlp(model):\n",
    "    for i in range(0, 24):\n",
    "        todo_layer = model.blocks[i]\n",
    "        linear_layers = get_linear_layers(todo_layer)[3:] # cross-channel sublayer only\n",
    "        cross_layer_equalization(linear_layers)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "\n",
    "    def __init__(self, name, fmt=':f'):\n",
    "        self.name = name\n",
    "        self.fmt = fmt\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "    def __str__(self):\n",
    "        fmtstr = '{name} {val' + self.fmt + '} ({avg' + self.fmt + '})'\n",
    "        return fmtstr.format(**self.__dict__)\n",
    "\n",
    "\n",
    "class ProgressMeter(object):\n",
    "    def __init__(self, num_batches, meters, prefix=\"\"):\n",
    "        self.batch_fmtstr = self._get_batch_fmtstr(num_batches)\n",
    "        self.meters = meters\n",
    "        self.prefix = prefix\n",
    "\n",
    "    def display(self, batch):\n",
    "        entries = [self.prefix + self.batch_fmtstr.format(batch)]\n",
    "        entries += [str(meter) for meter in self.meters]\n",
    "        print('\\t'.join(entries))\n",
    "\n",
    "    def _get_batch_fmtstr(self, num_batches):\n",
    "        num_digits = len(str(num_batches // 1))\n",
    "        fmt = '{:' + str(num_digits) + 'd}'\n",
    "        return '[' + fmt + '/' + fmt.format(num_batches) + ']'\n",
    "\n",
    "\n",
    "def calibrate(val_loader, model):\n",
    "    batch_time = AverageMeter('Time', ':6.3f')\n",
    "    progress = ProgressMeter(\n",
    "        len(val_loader),\n",
    "        [batch_time],\n",
    "        prefix='Calibrate: ')\n",
    "\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        end = time.time()\n",
    "        for i, (images, target) in enumerate(val_loader):\n",
    "            images = images.cuda()\n",
    "            target = target.cuda()\n",
    "\n",
    "            # compute output\n",
    "            output = model(images)\n",
    "\n",
    "            # measure elapsed time\n",
    "            batch_time.update(time.time() - end)\n",
    "            end = time.time()\n",
    "\n",
    "            if i % 10 == 0:\n",
    "                progress.display(i)\n",
    "\n",
    "    print('Calibration done.')\n",
    "\n",
    "def find_layers_dist(linear_layers):\n",
    "    layers_dist = {}\n",
    "    momentum = 0.1\n",
    "    def get_std_mean(name):\n",
    "        def hook(model, input, output):\n",
    "            new_std_mean = output.detach().torch.std_mean(x, dim=[0, 1], unbiased=False)\n",
    "            if name in layers_dist:\n",
    "                layers_dist[name][0] = (layers_dist[name][0]*momentum) + (new_std_mean[0]*(1 - momentum))\n",
    "                layers_dist[name][1] = (layers_dist[name][1]*momentum) + (new_std_mean[1]*(1 - momentum))\n",
    "            else:\n",
    "                layers_dist[name] = new_std_mean\n",
    "        return hook\n",
    "\n",
    "    # register hook\n",
    "    for i, (n, m) in enumerate(linear_layers):\n",
    "        m.register_forward_hook(get_std_mean(f'l{i}-{n}'))\n",
    "\n",
    "    # access small batch of validation data\n",
    "    data_loc = \"/mnt/disk1/imagenet\"\n",
    "    valdir = os.path.join(data_loc, 'val')\n",
    "    train_resolution = 224\n",
    "    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                     std=[0.229, 0.224, 0.225])\n",
    "    val_dataset = datasets.ImageFolder(\n",
    "        valdir,\n",
    "        transforms.Compose([\n",
    "            transforms.RandomResizedCrop(train_resolution),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            normalize,\n",
    "        ]))\n",
    "    \n",
    "    data_percentage = 0.0001\n",
    "    dataset_length = int(len(val_dataset) * data_percentage)\n",
    "    partial_train_dataset, _ = torch.utils.data.random_split(val_dataset,\n",
    "                                                                [dataset_length, len(val_dataset) - dataset_length])\n",
    "    val_loader = torch.utils.data.DataLoader(\n",
    "        partial_train_dataset, batch_size=32, shuffle=True,\n",
    "        num_workers=4, pin_memory=True, sampler=None)\n",
    "\n",
    "    calibrate(val_loader, model)\n",
    "\n",
    "    return layers_dist\n",
    "    \n",
    "\n",
    "def high_bias_absorption(linear_layers, layers_dist):\n",
    "    for idx in range(1, len(linear_layers)):\n",
    "        (prev_name, prev), (curr_name, curr) = linear_layers[idx-1], linear_layers[idx]\n",
    "        gamma, beta = layers_dist[idx-1]\n",
    "        # torch.std_mean(a, dim=1, unbiased=False)[0]\n",
    "\n",
    "def resmlp_bias_absorb(model):\n",
    "    linear_layers = []\n",
    "    for i in range(0, 24):\n",
    "        todo_layer = getattr(model, f'layer{i}')\n",
    "        linear_layers += get_linear_layers(todo_layer)[3:] # cross-channel sublayer only\n",
    "    print(len(linear_layers))\n",
    "    layers_dist = find_layers_dist(linear_layers)\n",
    "    high_bias_absorption(linear_layers, layers_dist)\n",
    "\n",
    "resmlp_bias_absorb(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weight/Bias Distribution Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_dist_layer(model, start, end):\n",
    "  plt.title('Weight Distribution of Each Layer')\n",
    "  plt.rcParams[\"figure.figsize\"] = [20, 5]\n",
    "  plt.rcParams[\"figure.autolayout\"] = True\n",
    "\n",
    "  data = []\n",
    "  labels = []\n",
    "  for i in range(start, end+1):\n",
    "    todo_layer = model.blocks[i]\n",
    "    tlist = get_linear_layers(todo_layer)\n",
    "\n",
    "    for j, (n, m) in enumerate(tlist):\n",
    "      val = m.weight.detach().numpy().flatten()\n",
    "      data.append(val)\n",
    "      labels.append(f'{i}')\n",
    "    \n",
    "  # Creating plot\n",
    "  bp = plt.boxplot(data, labels=labels)\n",
    "  plt.show()\n",
    "\n",
    "def bias_dist_layer(model, start, end):\n",
    "  plt.title('Bias Distribution of Each Layer')\n",
    "  plt.rcParams[\"figure.figsize\"] = [20, 5]\n",
    "  plt.rcParams[\"figure.autolayout\"] = True\n",
    "\n",
    "  data = []\n",
    "  labels = []\n",
    "  for i in range(start, end+1):\n",
    "    todo_layer = model.blocks[i]\n",
    "    tlist = get_linear_layers(todo_layer)\n",
    "\n",
    "    for j, (n, m) in enumerate(tlist):\n",
    "      if m.bias is not None:\n",
    "        val = m.bias.detach().numpy().flatten()\n",
    "        data.append(val)\n",
    "        labels.append(f'{i}')\n",
    "    \n",
    "  # Creating plot\n",
    "  bp = plt.boxplot(data, labels=labels)\n",
    "  plt.show()\n",
    "\n",
    "def weight_dist_gamma(model, start, end):\n",
    "  plt.title('Weight Distribution of Gamma_1/Gamma_2 for Each Layer')\n",
    "  plt.rcParams[\"figure.figsize\"] = [20, 5]\n",
    "  plt.rcParams[\"figure.autolayout\"] = True\n",
    "\n",
    "  data = []\n",
    "  labels = []\n",
    "  mdict = model.blocks.state_dict()\n",
    "  for i in range(start, end+1):\n",
    "    todo_layer = model.blocks[i]\n",
    "    data1 = todo_layer.gamma_1.weight.detach().numpy().flatten()\n",
    "    data2 = todo_layer.gamma_2.weight.detach().numpy().flatten()\n",
    "\n",
    "    # data = np.concatenate([[data1], [data2]], axis=0)\n",
    "    data.append(data1)\n",
    "    data.append(data2)\n",
    "    labels.append(f'{i}g1')\n",
    "    labels.append(f'{i}g2')\n",
    "    \n",
    "  # Creating plot\n",
    "  bp = plt.boxplot(data, labels=labels)\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = resmlp_24(pretrained=True).eval()\n",
    "# qmodel = q_resmlp24(model, full_precision_flag=True)\n",
    "weight_dist_gamma(model, 0, 23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cle_for_resmlp(model)\n",
    "# bias_dist_layer(model, 0, 23)\n",
    "weight_dist_gamma(model, 0, 23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.rcParams[\"figure.figsize\"] = [20, 5]\n",
    "\n",
    "# plt.rcParams[\"figure.autolayout\"] = True\n",
    "\n",
    "# layer_name = 'layer0.gamma_2.weight'\n",
    "# print(\"(min, max):  \", (qmdict[layer_name].min(), qmdict[layer_name].max()))\n",
    "# print(\"(std, mean): \", torch.std_mean(qmdict[layer_name], unbiased=False))\n",
    "# ax = sns.heatmap(qmdict[layer_name])\n",
    "# ax.plot()\n",
    "\n",
    "# layer_name = 'layer1.gamma_1.weight'\n",
    "# print(\"(min, max):  \", (qmdict[layer_name].min(), qmdict[layer_name].max()))\n",
    "# print(\"(std, mean): \", torch.std_mean(qmdict[layer_name], unbiased=False))\n",
    "# ax = sns.heatmap(qmdict[layer_name])\n",
    "# ax.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation Distribution Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "\n",
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                     std=[0.229, 0.224, 0.225])\n",
    "train_resolution = 224  \n",
    "transform = transforms.Compose([\n",
    "            transforms.ToPILImage(),\n",
    "            transforms.RandomResizedCrop(train_resolution),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            normalize,\n",
    "        ])\n",
    "\n",
    "# simulate input\n",
    "x = np.array(np.rint(np.random.rand(500, 375, 3) * 255), dtype=np.uint8)\n",
    "x = transform(x).unsqueeze(0)\n",
    "# cuda0 = torch.device('cuda:0')\n",
    "# x = x.to(cuda0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activations = {}\n",
    "def get_activation(name):\n",
    "    def hook(model, input, output):\n",
    "        activations[name] = output[0].detach()\n",
    "    return hook\n",
    "\n",
    "getattr(qmodel, 'quant_patch').norm.register_forward_hook(get_activation('in'))\n",
    "for i in range(0, 24):\n",
    "  layer_name = f'layer{i}'\n",
    "  layer1_name = f'{i}g1'\n",
    "  layer2_name = f'{i}g2'\n",
    "  getattr(qmodel, layer_name).gamma_1.register_forward_hook(get_activation(layer1_name))\n",
    "  getattr(qmodel, layer_name).gamma_2.register_forward_hook(get_activation(layer2_name))\n",
    "\n",
    "output = qmodel(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('Activation Distribution from Gamma_1/Gamma_2 for Each Layer')\n",
    "plt.rcParams[\"figure.figsize\"] = [5, 5]\n",
    "plt.rcParams[\"figure.autolayout\"] = True\n",
    "\n",
    "data = []\n",
    "labels = []\n",
    "\n",
    "from_layers=23\n",
    "to_layers=23\n",
    "for name in list(activations)[from_layers*2 : (to_layers+1)*2+1]:\n",
    "  labels.append(name)\n",
    "  # print(activations[name].shape)\n",
    "  data.append(activations[name][0].flatten())\n",
    "  \n",
    "# Creating plot\n",
    "bp = plt.boxplot(data, labels=labels)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QAT weight comparison test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for a in model.named_modules():\n",
    "  print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "class MyModel(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(MyModel, self).__init__()\n",
    "    self.my_tensor = torch.zeros(1) # 参数直接作为模型类成员变量\n",
    "    self.register_buffer('my_buffer', torch.zeros(1)) # 参数注册为 buffer\n",
    "    self.my_param = nn.Parameter(torch.zeros(1), requires_grad=False)\n",
    "\n",
    "  def forward(self):\n",
    "    self.my_tensor = torch.ones(1)\n",
    "    self.my_param = nn.Parameter(torch.ones(1), requires_grad=False)\n",
    "    self.my_buffer = torch.ones(1)\n",
    "    return\n",
    "\n",
    "model = MyModel()\n",
    "model()\n",
    "\n",
    "print(model.state_dict())\n",
    "model.cuda()\n",
    "print(model.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(\"model_best.pth.tar\")['state_dict']\n",
    "# print(checkpoint)\n",
    "modified_dict = {}\n",
    "for key, value in checkpoint.items():\n",
    "    if 'num_batches_tracked' in key: continue\n",
    "    # if 'weight_integer' in key: continue\n",
    "    # if 'bias_integer' in key: continue\n",
    "\n",
    "    modified_key = key.replace(\"module.\", \"\")\n",
    "    modified_dict[modified_key] = value\n",
    "qmodel.load_state_dict(modified_dict, strict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "org_model = resmlp_24(pretrained=True)\n",
    "org_qmodel = q_resmlp24(org_model, full_precision_flag=True)\n",
    "\n",
    "torch.equal(qmodel.layer0.gamma_1.linear.weight, org_qmodel.layer0.gamma_1.linear.weight)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
